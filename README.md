# Mitigating Dynamic Gaps in Policy Transfer Through Trajectory Alignment with Model-based Reinforcement Learning and Dynamic Time Warping

This project presents an approach for transferring policies between two environments characterized by dynamic gaps, where the target environment lacks inherent rewards. The proposed method utilizes model-based reinforcement learning (RL), specifically the Dreamer algorithm, to determine the optimal trajectory within the target environment that corresponds with the trajectory from the source environment. Dynamic Time Warping (DTW) serves as a primary metric for evaluating similarity between trajectories to facilitate effective alignment, enabling the consideration of temporal fluctuations and disparities in dynamics between the two environments.   Furthermore, the approach incorporates a reward calculation mechanism based on aligning observations from the source environment, facilitating improved policy adaptation in the target domain. This is particularly crucial given the absence of rewards in the target environment.
